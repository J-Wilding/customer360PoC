{
	"name": "Customer360_PoC",
	"properties": {
		"activities": [
			{
				"name": "generateChurn",
				"type": "ExecuteDataFlow",
				"dependsOn": [
					{
						"activity": "ETL",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"policy": {
					"timeout": "1.00:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"dataflow": {
						"referenceName": "generateChurn",
						"type": "DataFlowReference"
					},
					"compute": {
						"coreCount": 8,
						"computeType": "General"
					},
					"traceLevel": "Fine"
				}
			},
			{
				"name": "UpdateDashboard",
				"type": "DatabricksNotebook",
				"dependsOn": [
					{
						"activity": "generateChurn",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"policy": {
					"timeout": "7.00:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"notebookPath": "/Users/jonathan.wilding@hcl.com/Customer360PoC/churn_prediction"
				},
				"linkedServiceName": {
					"referenceName": "AzureDatabricks1",
					"type": "LinkedServiceReference"
				}
			},
			{
				"name": "ChurnPrediction",
				"type": "DatabricksNotebook",
				"dependsOn": [
					{
						"activity": "generateChurn",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"policy": {
					"timeout": "7.00:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"notebookPath": "/Users/jonathan.wilding@hcl.com/Customer360PoC/churn_prediction"
				},
				"linkedServiceName": {
					"referenceName": "AzureDatabricks1",
					"type": "LinkedServiceReference"
				}
			},
			{
				"name": "ETL",
				"description": "During the PoC I had a prep step converting CSV to parquet which I realized after the fact is inefficient since it loads to memory twice. Though since the data is now in parquet format it is more efficient to read this rather than use the CSV. In the production pipeline, I would revert to using the CSV rather than the parquet since all subsequent data would be ingested via CSV.",
				"type": "ExecutePipeline",
				"dependsOn": [],
				"userProperties": [],
				"typeProperties": {
					"pipeline": {
						"referenceName": "ETL",
						"type": "PipelineReference"
					},
					"waitOnCompletion": true
				}
			}
		],
		"annotations": [],
		"lastPublishTime": "2022-01-07T21:36:38Z"
	},
	"type": "Microsoft.DataFactory/factories/pipelines"
}